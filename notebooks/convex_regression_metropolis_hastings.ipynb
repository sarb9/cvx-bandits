{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Regression using Metropolis-Hastings Algorithm\n",
    "\n",
    "In this notebook, we'll implement a Bayesian approach to convex regression using the Metropolis-Hastings algorithm. Convex regression is particularly useful when we have prior knowledge that the function we're trying to fit is convex, which is common in many economic and optimization problems.\n",
    "\n",
    "## Theory and Background\n",
    "\n",
    "### Convex Regression\n",
    "\n",
    "A function $f: \\mathbb{R}^d \\to \\mathbb{R}$ is convex if for all $x, y \\in \\mathbb{R}^d$ and $\\lambda \\in [0, 1]$:\n",
    "\n",
    "$$f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)$$\n",
    "\n",
    "In the context of regression, we want to find a function $f$ that fits our data $(x_i, y_i)$ while ensuring that $f$ is convex. \n",
    "\n",
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "The Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a probability distribution. The key steps are:\n",
    "\n",
    "1. Start with an initial state $\\theta_0$\n",
    "2. For each iteration $t$:\n",
    "   - Propose a new state $\\theta'$ from a proposal distribution $q(\\theta' | \\theta_t)$\n",
    "   - Calculate the acceptance ratio $\\alpha = \\min\\left(1, \\frac{p(\\theta')q(\\theta_t|\\theta')}{p(\\theta_t)q(\\theta'|\\theta_t)}\\right)$\n",
    "   - Accept the proposed state with probability $\\alpha$\n",
    "\n",
    "For convex regression, we'll use this algorithm to sample from the posterior distribution of the parameters of our model, subject to convexity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "First, let's generate some synthetic data from a known convex function with added noise. We'll use a quadratic function as our ground truth since it's guaranteed to be convex when the coefficient of the quadratic term is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=100, noise_level=0.5, x_range=(-5, 5)):\n",
    "    \"\"\"\n",
    "    Generate synthetic data from a convex function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Number of data points\n",
    "    noise_level : float\n",
    "        Standard deviation of the Gaussian noise\n",
    "    x_range : tuple\n",
    "        Range of the x values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : array, shape (n,)\n",
    "        Input features\n",
    "    y : array, shape (n,)\n",
    "        Output values\n",
    "    true_function : function\n",
    "        The ground truth function\n",
    "    \"\"\"\n",
    "    # Generate x values\n",
    "    X = np.random.uniform(x_range[0], x_range[1], n)\n",
    "    \n",
    "    # Define a convex function (quadratic)\n",
    "    def true_function(x):\n",
    "        return 0.5 * x**2 + 1.0 * x + 2.0\n",
    "    \n",
    "    # Generate y values with noise\n",
    "    y_true = true_function(X)\n",
    "    y = y_true + np.random.normal(0, noise_level, n)\n",
    "    \n",
    "    return X, y, true_function\n",
    "\n",
    "# Generate data\n",
    "X, y, true_function = generate_data(n=100, noise_level=1.0)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Observed data')\n",
    "x_line = np.linspace(-5, 5, 1000)\n",
    "plt.plot(x_line, true_function(x_line), 'r-', label='True function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Data from a Convex Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Polynomial Regression with Convexity Constraints\n",
    "\n",
    "We'll use a polynomial regression model as the basis for our convex regression. For a univariate polynomial of degree $d$, the convexity constraint is simply that the second derivative is non-negative throughout the domain of interest.\n",
    "\n",
    "For a polynomial $f(x) = \\sum_{i=0}^{d} \\beta_i x^i$, the second derivative is $f''(x) = \\sum_{i=2}^{d} i(i-1)\\beta_i x^{i-2}$.\n",
    "\n",
    "The convexity constraint means $f''(x) \\geq 0$ for all $x$ in our domain. This is a complex constraint to enforce directly, but we can check it at a finite number of points in our domain.\n",
    "\n",
    "### Model Setup\n",
    "\n",
    "We'll define the likelihood function, prior distributions, and the Metropolis-Hastings algorithm for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convexity(coeffs, x_test):\n",
    "    \"\"\"\n",
    "    Check if a polynomial with given coefficients is convex at test points\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    coeffs : array\n",
    "        Polynomial coefficients in ascending order [constant, x, x^2, ...]\n",
    "    x_test : array\n",
    "        Points to test convexity at\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    is_convex : bool\n",
    "        True if the polynomial is convex at all test points\n",
    "    \"\"\"\n",
    "    # For polynomials, we need the coefficient of x^2 to be positive\n",
    "    # and for higher degrees, we need to check the second derivative\n",
    "    \n",
    "    # Get the degree of the polynomial\n",
    "    degree = len(coeffs) - 1\n",
    "    \n",
    "    if degree < 2:\n",
    "        # Linear or constant functions are always convex\n",
    "        return True\n",
    "    \n",
    "    # Check if the coefficient of x^2 is positive\n",
    "    if coeffs[2] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # For higher degrees, check the second derivative at test points\n",
    "    if degree > 2:\n",
    "        # Compute the coefficients of the second derivative\n",
    "        second_deriv_coeffs = np.zeros(degree - 1)\n",
    "        for i in range(2, degree + 1):\n",
    "            second_deriv_coeffs[i-2] = i * (i-1) * coeffs[i]\n",
    "        \n",
    "        # Evaluate the second derivative at test points\n",
    "        for x in x_test:\n",
    "            second_deriv = 0\n",
    "            for i, coef in enumerate(second_deriv_coeffs):\n",
    "                second_deriv += coef * x**(i)\n",
    "            \n",
    "            if second_deriv < 0:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def log_likelihood(X, y, coeffs, sigma):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of the data given the model parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (n,)\n",
    "        Input features\n",
    "    y : array, shape (n,)\n",
    "        Output values\n",
    "    coeffs : array\n",
    "        Polynomial coefficients\n",
    "    sigma : float\n",
    "        Standard deviation of the noise\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    log_like : float\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    # Compute the predicted values\n",
    "    y_pred = np.zeros_like(y)\n",
    "    for i, x in enumerate(X):\n",
    "        for j, coef in enumerate(coeffs):\n",
    "            y_pred[i] += coef * x**j\n",
    "    \n",
    "    # Compute the log-likelihood\n",
    "    log_like = -0.5 * np.sum((y - y_pred)**2) / (sigma**2)\n",
    "    log_like -= 0.5 * len(X) * np.log(2 * np.pi * sigma**2)\n",
    "    \n",
    "    return log_like\n",
    "\n",
    "def log_prior(coeffs, sigma):\n",
    "    \"\"\"\n",
    "    Compute the log-prior of the model parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    coeffs : array\n",
    "        Polynomial coefficients\n",
    "    sigma : float\n",
    "        Standard deviation of the noise\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    log_prior : float\n",
    "        Log-prior value\n",
    "    \"\"\"\n",
    "    # Use a normal prior for the coefficients\n",
    "    log_prior_coeffs = np.sum(stats.norm.logpdf(coeffs, loc=0, scale=10))\n",
    "    \n",
    "    # Use an inverse gamma prior for sigma^2\n",
    "    log_prior_sigma = stats.invgamma.logpdf(sigma**2, a=1, scale=1)\n",
    "    \n",
    "    return log_prior_coeffs + log_prior_sigma\n",
    "\n",
    "def log_posterior(X, y, coeffs, sigma, x_test):\n",
    "    \"\"\"\n",
    "    Compute the log-posterior of the model parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (n,)\n",
    "        Input features\n",
    "    y : array, shape (n,)\n",
    "        Output values\n",
    "    coeffs : array\n",
    "        Polynomial coefficients\n",
    "    sigma : float\n",
    "        Standard deviation of the noise\n",
    "    x_test : array\n",
    "        Points to test convexity at\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    log_post : float\n",
    "        Log-posterior value\n",
    "    \"\"\"\n",
    "    # Check convexity constraint\n",
    "    if not check_convexity(coeffs, x_test):\n",
    "        return -np.inf  # Reject non-convex functions\n",
    "    \n",
    "    # Compute log-posterior as log-likelihood + log-prior\n",
    "    log_post = log_likelihood(X, y, coeffs, sigma) + log_prior(coeffs, sigma)\n",
    "    \n",
    "    return log_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metropolis-Hastings Algorithm for Convex Regression\n",
    "\n",
    "Now we'll implement the Metropolis-Hastings algorithm to sample from the posterior distribution of the model parameters, subject to the convexity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(X, y, degree=3, n_iterations=10000, burn_in=1000, x_test=np.linspace(-5, 5, 100)):\n",
    "    \"\"\"\n",
    "    Perform Metropolis-Hastings sampling for convex regression\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (n,)\n",
    "        Input features\n",
    "    y : array, shape (n,)\n",
    "        Output values\n",
    "    degree : int\n",
    "        Degree of the polynomial\n",
    "    n_iterations : int\n",
    "        Number of iterations\n",
    "    burn_in : int\n",
    "        Number of burn-in iterations to discard\n",
    "    x_test : array\n",
    "        Points to test convexity at\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : dict\n",
    "        Dictionary containing the samples and other information\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    coeffs = np.zeros(degree + 1)\n",
    "    coeffs[0] = np.mean(y)  # Intercept term\n",
    "    if degree >= 2:\n",
    "        coeffs[2] = 0.1  # Small positive coefficient for x^2 to ensure initial convexity\n",
    "    \n",
    "    sigma = np.std(y)\n",
    "    \n",
    "    # Initialize arrays to store samples\n",
    "    coeffs_samples = np.zeros((n_iterations, degree + 1))\n",
    "    sigma_samples = np.zeros(n_iterations)\n",
    "    log_post_samples = np.zeros(n_iterations)\n",
    "    acceptance_rate = 0\n",
    "    \n",
    "    # Initial log-posterior\n",
    "    current_log_post = log_posterior(X, y, coeffs, sigma, x_test)\n",
    "    \n",
    "    # Run Metropolis-Hastings\n",
    "    for i in range(n_iterations):\n",
    "        # Propose new coefficients\n",
    "        proposed_coeffs = coeffs + np.random.normal(0, 0.1, size=degree + 1)\n",
    "        \n",
    "        # Propose new sigma\n",
    "        proposed_sigma = sigma * np.exp(np.random.normal(0, 0.1))\n",
    "        \n",
    "        # Compute log-posterior for the proposed parameters\n",
    "        proposed_log_post = log_posterior(X, y, proposed_coeffs, proposed_sigma, x_test)\n",
    "        \n",
    "        # Compute acceptance ratio (in log space)\n",
    "        log_acceptance_ratio = proposed_log_post - current_log_post\n",
    "        \n",
    "        # Accept or reject\n",
    "        if np.log(np.random.uniform()) < log_acceptance_ratio:\n",
    "            coeffs = proposed_coeffs\n",
    "            sigma = proposed_sigma\n",
    "            current_log_post = proposed_log_post\n",
    "            if i >= burn_in:\n",
    "                acceptance_rate += 1\n",
    "        \n",
    "        # Store samples\n",
    "        coeffs_samples[i] = coeffs\n",
    "        sigma_samples[i] = sigma\n",
    "        log_post_samples[i] = current_log_post\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 1000 == 0 or i == 0:\n",
    "            print(f\"Iteration {i+1}/{n_iterations}, log-posterior: {current_log_post:.2f}\")\n",
    "    \n",
    "    # Compute acceptance rate for post burn-in samples\n",
    "    acceptance_rate /= (n_iterations - burn_in)\n",
    "    \n",
    "    # Return samples and other information\n",
    "    return {\n",
    "        'coeffs': coeffs_samples[burn_in:],\n",
    "        'sigma': sigma_samples[burn_in:],\n",
    "        'log_post': log_post_samples[burn_in:],\n",
    "        'acceptance_rate': acceptance_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Metropolis-Hastings Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data for easier visualization\n",
    "sort_idx = np.argsort(X)\n",
    "X_sorted = X[sort_idx]\n",
    "y_sorted = y[sort_idx]\n",
    "\n",
    "# Run Metropolis-Hastings\n",
    "samples = metropolis_hastings(\n",
    "    X=X, \n",
    "    y=y, \n",
    "    degree=3, \n",
    "    n_iterations=5000,  # For demonstration, use a smaller number\n",
    "    burn_in=1000,\n",
    "    x_test=np.linspace(-5, 5, 100)\n",
    ")\n",
    "\n",
    "print(f\"Acceptance rate: {samples['acceptance_rate']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results\n",
    "\n",
    "Now let's analyze the posterior samples and visualize the resulting convex regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trace of the coefficients\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(samples['coeffs'].shape[1]):\n",
    "    plt.subplot(samples['coeffs'].shape[1], 1, i+1)\n",
    "    plt.plot(samples['coeffs'][:, i])\n",
    "    plt.ylabel(f'$\\\\beta_{i}$')\n",
    "    if i == samples['coeffs'].shape[1] - 1:\n",
    "        plt.xlabel('Iteration')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Trace of Polynomial Coefficients')\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()\n",
    "\n",
    "# Plot the trace of sigma\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(samples['sigma'])\n",
    "plt.ylabel('$\\\\sigma$')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Trace of Noise Standard Deviation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the posterior distribution of the coefficients\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(samples['coeffs'].shape[1]):\n",
    "    plt.subplot(samples['coeffs'].shape[1], 1, i+1)\n",
    "    sns.histplot(samples['coeffs'][:, i], kde=True)\n",
    "    plt.xlabel(f'$\\\\beta_{i}$')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Posterior Distribution of Polynomial Coefficients')\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the posterior mean of the coefficients\n",
    "mean_coeffs = np.mean(samples['coeffs'], axis=0)\n",
    "print(\"Posterior mean of coefficients:\")\n",
    "for i, coef in enumerate(mean_coeffs):\n",
    "    print(f\"β_{i}: {coef:.4f}\")\n",
    "\n",
    "# Compute the posterior mean of sigma\n",
    "mean_sigma = np.mean(samples['sigma'])\n",
    "print(f\"\\nPosterior mean of σ: {mean_sigma:.4f}\")\n",
    "\n",
    "# Compute the true coefficients for comparison\n",
    "print(\"\\nTrue coefficients:\")\n",
    "print(f\"β_0: {2.0:.4f}\")\n",
    "print(f\"β_1: {1.0:.4f}\")\n",
    "print(f\"β_2: {0.5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize the Convex Regression Fit\n",
    "\n",
    "Now let's visualize the posterior mean fit and the uncertainty bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for prediction\n",
    "x_pred = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Function to evaluate the polynomial\n",
    "def eval_poly(x, coeffs):\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    for i, coef in enumerate(coeffs):\n",
    "        y += coef * x**i\n",
    "    return y\n",
    "\n",
    "# Compute the posterior mean prediction\n",
    "y_pred_mean = eval_poly(x_pred, mean_coeffs)\n",
    "\n",
    "# Compute the 95% credible interval\n",
    "n_samples = min(100, len(samples['coeffs']))  # Use subset of samples for efficiency\n",
    "y_pred_samples = np.zeros((n_samples, len(x_pred)))\n",
    "for i in range(n_samples):\n",
    "    idx = np.random.randint(0, len(samples['coeffs']))\n",
    "    coeffs_sample = samples['coeffs'][idx]\n",
    "    sigma_sample = samples['sigma'][idx]\n",
    "    y_pred_samples[i] = eval_poly(x_pred, coeffs_sample)\n",
    "\n",
    "# Compute percentiles for credible interval\n",
    "y_pred_lower = np.percentile(y_pred_samples, 2.5, axis=0)\n",
    "y_pred_upper = np.percentile(y_pred_samples, 97.5, axis=0)\n",
    "\n",
    "# Plot the data and the fit\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X, y, alpha=0.6, label='Observed data')\n",
    "plt.plot(x_pred, y_pred_mean, 'r-', linewidth=2, label='Posterior mean')\n",
    "plt.fill_between(x_pred, y_pred_lower, y_pred_upper, alpha=0.2, color='r', label='95% credible interval')\n",
    "plt.plot(x_pred, true_function(x_pred), 'g--', linewidth=2, label='True function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Convex Regression using Metropolis-Hastings')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Posterior Predictive Checks\n",
    "\n",
    "Let's perform some posterior predictive checks to assess the fit of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples\n",
    "n_pp_samples = 100\n",
    "pp_samples = np.zeros((n_pp_samples, len(X)))\n",
    "\n",
    "for i in range(n_pp_samples):\n",
    "    idx = np.random.randint(0, len(samples['coeffs']))\n",
    "    coeffs_sample = samples['coeffs'][idx]\n",
    "    sigma_sample = samples['sigma'][idx]\n",
    "    \n",
    "    # Compute the mean prediction\n",
    "    y_mean = eval_poly(X, coeffs_sample)\n",
    "    \n",
    "    # Add noise\n",
    "    pp_samples[i] = y_mean + np.random.normal(0, sigma_sample, size=len(X))\n",
    "\n",
    "# Plot the posterior predictive distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the original data\n",
    "plt.scatter(X, y, alpha=0.6, color='blue', label='Observed data')\n",
    "\n",
    "# Plot some posterior predictive samples\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, n_pp_samples)\n",
    "    plt.scatter(X, pp_samples[idx], alpha=0.2, color='red')\n",
    "\n",
    "plt.plot(x_pred, true_function(x_pred), 'g--', linewidth=2, label='True function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Posterior Predictive Check')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare the distribution of residuals between the observed data and posterior predictive samples\n",
    "y_pred_obs = eval